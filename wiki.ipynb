{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-3.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting filelock (from datasets)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting numpy>=1.17 (from datasets)\n",
      "  Using cached numpy-2.2.3-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-19.0.1-cp313-cp313-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.2.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.11.13-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Using cached huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /Users/aadil/Documents/Workspace/ml-projects/wiki-llm/.venv/lib/python3.13/site-packages (from datasets) (24.2)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Using cached PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.5.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.1.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Using cached propcache-0.3.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.18.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (69 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.24.0->datasets)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.32.2->datasets)\n",
      "  Using cached charset_normalizer-3.4.1-cp313-cp313-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.32.2->datasets)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.32.2->datasets)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/aadil/Documents/Workspace/ml-projects/wiki-llm/.venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/aadil/Documents/Workspace/ml-projects/wiki-llm/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached datasets-3.4.0-py3-none-any.whl (487 kB)\n",
      "Using cached transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Using cached aiohttp-3.11.13-cp313-cp313-macosx_11_0_arm64.whl (453 kB)\n",
      "Using cached huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached numpy-2.2.3-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Using cached pyarrow-19.0.1-cp313-cp313-macosx_12_0_arm64.whl (30.7 MB)\n",
      "Using cached PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl (171 kB)\n",
      "Using cached regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached pandas-2.2.3-cp313-cp313-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached xxhash-3.5.0-cp313-cp313-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp313-cp313-macosx_10_13_universal2.whl (195 kB)\n",
      "Using cached frozenlist-1.5.0-cp313-cp313-macosx_11_0_arm64.whl (50 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached multidict-6.1.0-cp313-cp313-macosx_11_0_arm64.whl (29 kB)\n",
      "Using cached propcache-0.3.0-cp313-cp313-macosx_11_0_arm64.whl (44 kB)\n",
      "Using cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached yarl-1.18.3-cp313-cp313-macosx_11_0_arm64.whl (91 kB)\n",
      "Installing collected packages: pytz, xxhash, urllib3, tzdata, typing-extensions, tqdm, safetensors, regex, pyyaml, pyarrow, propcache, numpy, multidict, idna, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, attrs, aiohappyeyeballs, yarl, requests, pandas, multiprocess, aiosignal, huggingface-hub, aiohttp, tokenizers, transformers, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.13 aiosignal-1.3.2 attrs-25.3.0 certifi-2025.1.31 charset-normalizer-3.4.1 datasets-3.4.0 dill-0.3.8 filelock-3.18.0 frozenlist-1.5.0 fsspec-2024.12.0 huggingface-hub-0.29.3 idna-3.10 multidict-6.1.0 multiprocess-0.70.16 numpy-2.2.3 pandas-2.2.3 propcache-0.3.0 pyarrow-19.0.1 pytz-2025.1 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.3 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.49.0 typing-extensions-4.12.2 tzdata-2025.1 urllib3-2.3.0 xxhash-3.5.0 yarl-1.18.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the model hyperparameters\n",
    "n_embd = 768\n",
    "n_head = 12\n",
    "n_layer = 12\n",
    "max_len = 512\n",
    "dropout = 0.2\n",
    "block_size = 64\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def get_max_length(dataset, tokenizer):\n",
    "    def token_length(example):\n",
    "        return {\"length\": len(tokenizer(example[\"text\"])[\"input_ids\"])}\n",
    "\n",
    "    dataset_with_lengths = dataset.map(token_length)\n",
    "    max_length = max(dataset_with_lengths[\"length\"])\n",
    "    return max_length\n",
    "\n",
    "fixed_max_len = True\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=max_len\n",
    "    )\n",
    "\n",
    "# ds = ds.map(tokenize_function, batched=True)\n",
    "tokenized_dataset_path = \"wiki_10k_tokenized\"\n",
    "\n",
    "if not os.path.exists(tokenized_dataset_path):\n",
    "    print(\"Local cache of dataset not found, downloading and tokenizing dataset...\")\n",
    "    # Load dataset (small subset of 10,000 samples)\n",
    "    ds = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=\"train\")\n",
    "    ds = ds.shuffle(seed=42).select(range(10000))  # Select only 10,000 samples\n",
    "    # Select only the 'text' column\n",
    "    ds = ds.remove_columns([col for col in ds.column_names if col != \"text\"])\n",
    "    # Tokenize the dataset\n",
    "    ds = ds.map(tokenize_function, batched=True)\n",
    "    ds.save_to_disk(tokenized_dataset_path)\n",
    "else:\n",
    "    print(\"Local cache of dataset found, loading tokenized dataset...\")\n",
    "    ds = load_from_disk(tokenized_dataset_path)\n",
    "\n",
    "max_len = get_max_length(ds, tokenizer) if not fixed_max_len else 512\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = DataLoader(\n",
    "    ds.with_format('torch'),\n",
    "    batch_size=8,\n",
    "    collate_fn=lambda x: torch.stack([d['input_ids'] for d in x])\n",
    ")\n",
    "\n",
    "ex = next(iter(dataloader))\n",
    "print(ex)\n",
    "print(ex.shape)\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "n_embd = 768\n",
    "nn.Embedding(vocab_size, n_embd)(ex).shape\n",
    "\n",
    "eval_iters = 100  # Define eval_iters with an appropriate value\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def estimate_loss():\n",
    "#     out = {}\n",
    "#     model.eval()\n",
    "#     for split in ['train', 'val']:\n",
    "#         losses = torch.zeros(eval_iters)\n",
    "#         for k in range(eval_iters):\n",
    "#             X, Y = get_batch(split)\n",
    "#             logits, loss = model(X, Y)\n",
    "#             losses[k] = loss.item()\n",
    "#         out[split] = losses.mean()\n",
    "#     model.train()\n",
    "#     return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, C)\n",
    "        q = self.query(x) # (B, T, C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B, T, C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class WikiCompleteModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, n_head, n_layer, max_len, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, n_embd)\n",
    "        self.decoder = nn.TransformerDecoderx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS token ID: None\n",
      "CLS token ID: 101\n"
     ]
    }
   ],
   "source": [
    "bos_token_id = tokenizer.bos_token_id\n",
    "cls_token_id = tokenizer.cls_token_id\n",
    "\n",
    "print(\"BOS token ID:\", bos_token_id)\n",
    "print(\"CLS token ID:\", cls_token_id) # every sequence starts with this token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_dataset(tokenize_function):\n",
    "    tokenized_dataset_path = \"wiki_10k_tokenized\"\n",
    "\n",
    "    if not os.path.exists(tokenized_dataset_path):\n",
    "        print(\"Local cache of dataset not found, downloading and tokenizing dataset...\")\n",
    "        # Load dataset (small subset of 10,000 samples)\n",
    "        ds = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=\"train\")\n",
    "        ds = ds.shuffle(seed=42).select(range(10000))  # Select only 10,000 samples\n",
    "        # Select only the 'text' column\n",
    "        ds = ds.remove_columns([col for col in ds.column_names if col != \"text\"])\n",
    "        # Tokenize the dataset\n",
    "        ds = ds.map(tokenize_function, batched=True)\n",
    "        ds.save_to_disk(tokenized_dataset_path)\n",
    "    else:\n",
    "        print(\"Local cache of dataset found, loading tokenized dataset...\")\n",
    "        ds = load_from_disk(tokenized_dataset_path)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(examples, block_size=128):\n",
    "    # Get the tokenized input ids (should be a list of integers)\n",
    "    input_ids = examples[\"input_ids\"]\n",
    "    \n",
    "    # Initialize lists to store the new sequences and labels\n",
    "    input_sequences = []\n",
    "    label_sequences = []\n",
    "    \n",
    "    # Ensure input_ids is a list of integers\n",
    "    if isinstance(input_ids, list):\n",
    "        # Loop through the input sequences and split them into blocks\n",
    "        for i in range(len(input_ids) - block_size):\n",
    "            input_seq = input_ids[i:i + block_size]\n",
    "            label_seq = input_ids[i + block_size]  # The next token is the label\n",
    "            \n",
    "            input_sequences.append(input_seq)\n",
    "            label_sequences.append(label_seq)\n",
    "    \n",
    "    # Return the input sequences and labels as dictionaries\n",
    "    return {\"input_ids\": input_sequences, \"labels\": label_sequences}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local cache of dataset found, loading tokenized dataset...\n"
     ]
    }
   ],
   "source": [
    "tds = get_tokenized_dataset(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 3840000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Assuming `tds` is your tokenized dataset with columns ['input_ids']\n",
    "# You can extract the data as a list of dictionaries\n",
    "data = [{\"input_ids\": row[\"input_ids\"]} for row in tds]\n",
    "\n",
    "# Apply the function to create sequences for all rows\n",
    "new_data = [create_sequences(ex, block_size=128) for ex in data]\n",
    "\n",
    "# Now, we can create a new Dataset\n",
    "tokenized_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": [seq for ex in new_data for seq in ex[\"input_ids\"]],\n",
    "    \"labels\": [seq for ex in new_data for seq in ex[\"labels\"]],\n",
    "})\n",
    "\n",
    "# Print the dataset\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 20287, 2361, 6738, 2003, 1037, 4696, 1038, 2273, 1005, 1055, 2334, 3827, 2284, 1999, 9803, 2588, 6738, 1999, 2563, 1012, 1996, 2744, 1005, 2334, 1005, 2965, 2008, 2023, 3827, 4324, 2111, 2006, 2128, 2386, 2094, 2000, 1996, 2334, 5434, 1012, 1996, 3827, 2003, 3498, 2011, 2010, 9995, 1005, 1055, 3827, 2326, 1012, 2381, 6738, 3827, 2441, 1999, 6940, 1010, 1998, 2003, 1997, 1037, 5171, 6652, 2640, 1012, 19180, 2350, 2001, 1996, 2197, 2711, 1998, 2069, 2450, 2000, 2022, 6472, 2012, 6738, 1999, 4579, 1012, 2016, 2018, 2042, 7979, 1997, 1996, 4028, 1997, 2014, 3129, 1012, 2019, 4538, 1000, 2306, 2122, 3681, 1000, 4076, 1996, 3827, 1005, 1055, 2381, 2013, 14378, 2683, 2000, 4579, 1012, 1996, 4538, 2001, 2881, 1998, 2580, 2011, 2961, 6487, 16955, 1998, 3985, 2441], 'labels': 2011}\n",
      "{'input_ids': [20287, 2361, 6738, 2003, 1037, 4696, 1038, 2273, 1005, 1055, 2334, 3827, 2284, 1999, 9803, 2588, 6738, 1999, 2563, 1012, 1996, 2744, 1005, 2334, 1005, 2965, 2008, 2023, 3827, 4324, 2111, 2006, 2128, 2386, 2094, 2000, 1996, 2334, 5434, 1012, 1996, 3827, 2003, 3498, 2011, 2010, 9995, 1005, 1055, 3827, 2326, 1012, 2381, 6738, 3827, 2441, 1999, 6940, 1010, 1998, 2003, 1997, 1037, 5171, 6652, 2640, 1012, 19180, 2350, 2001, 1996, 2197, 2711, 1998, 2069, 2450, 2000, 2022, 6472, 2012, 6738, 1999, 4579, 1012, 2016, 2018, 2042, 7979, 1997, 1996, 4028, 1997, 2014, 3129, 1012, 2019, 4538, 1000, 2306, 2122, 3681, 1000, 4076, 1996, 3827, 1005, 1055, 2381, 2013, 14378, 2683, 2000, 4579, 1012, 1996, 4538, 2001, 2881, 1998, 2580, 2011, 2961, 6487, 16955, 1998, 3985, 2441, 2011], 'labels': 5623}\n",
      "{'input_ids': [2361, 6738, 2003, 1037, 4696, 1038, 2273, 1005, 1055, 2334, 3827, 2284, 1999, 9803, 2588, 6738, 1999, 2563, 1012, 1996, 2744, 1005, 2334, 1005, 2965, 2008, 2023, 3827, 4324, 2111, 2006, 2128, 2386, 2094, 2000, 1996, 2334, 5434, 1012, 1996, 3827, 2003, 3498, 2011, 2010, 9995, 1005, 1055, 3827, 2326, 1012, 2381, 6738, 3827, 2441, 1999, 6940, 1010, 1998, 2003, 1997, 1037, 5171, 6652, 2640, 1012, 19180, 2350, 2001, 1996, 2197, 2711, 1998, 2069, 2450, 2000, 2022, 6472, 2012, 6738, 1999, 4579, 1012, 2016, 2018, 2042, 7979, 1997, 1996, 4028, 1997, 2014, 3129, 1012, 2019, 4538, 1000, 2306, 2122, 3681, 1000, 4076, 1996, 3827, 1005, 1055, 2381, 2013, 14378, 2683, 2000, 4579, 1012, 1996, 4538, 2001, 2881, 1998, 2580, 2011, 2961, 6487, 16955, 1998, 3985, 2441, 2011, 5623], 'labels': 2350}\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for ex in tokenized_dataset:\n",
    "    print(ex)\n",
    "    if i < 2:\n",
    "        i += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mini-llm/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local cache of sliding window dataset not found, creating dataset...\n",
      "Local cache of dataset not found, downloading and tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 25901.96 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 83055/83055 [00:00<00:00, 1590675.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from data import WikipediaDataset\n",
    "from transformers import AutoTokenizer\n",
    "batch_size = 32\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "dataset = WikipediaDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=1024,\n",
    "    block_size=128, \n",
    "    regenerate=True,\n",
    "    num_samples=100\n",
    ")\n",
    "train_dataloader = dataset.get_test_train_dataloaders(\"train\", batch_size)\n",
    "val_dataloader = dataset.get_test_train_dataloaders(\"val\", batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Achilleía), attested in Attica in the fourth century BC (IG II² 1617) and, in the form Achillia, on a stele in Halicarnassus as the name of a female gladiator fighting an \"Amazon\".\n",
      "\n",
      "Achilles' name can be analyzed as a combination of  () \"distress, pain, sorrow, grief\" and  () \"people, soldiers, nation\", resulting in a proto-form *Akhí-lāu̯os \"he who has the people distressed\" or \"he whose people have distress\". The grief or distress of\n",
      "eskjöld and student of both Hugh Akston and Robert Stadler. He began working while still in school to show that he could have been successful without the aid of his family's wealth. Later, Francisco bankrupts the d'Anconia business to put it out of others' reach. His full name is given as \"Francisco Domingo Carlos Andres Sebastián d'Anconia\".\n",
      "\n",
      "John Galt\n",
      "\n",
      "John Galt is the primary male hero of Atlas Shrugged. He initially appears as an unnamed menial worker for Taggart Transcontinental, who often dines with Eddie\n",
      ", automobiles, swimming pools, and other patient support systems (i.e., radiology, surgical, or examining tables).\n",
      "\n",
      "The most common devices are transfer benches, stretcher or convertible chairs (for lateral, supine transfer), sit-to-stand lifts (for moving patients from one seated position to another i.e., from wheelchairs to commodes), air bearing inflatable mattresses (for supine transfer i.e., transfer from a gurney to an operating room table),  gait belts (or transfer belt) and a slider board (or transfer board), usually used for transfer from a bed to\n",
      " substances having the property of an acid are said to be acidic.\n",
      "\n",
      "Common aqueous acids include hydrochloric acid (a solution of hydrogen chloride that is found in gastric acid in the stomach and activates digestive enzymes), acetic acid (vinegar is a dilute aqueous solution of this liquid), sulfuric acid (used in car batteries), and citric acid (found in citrus fruits). As these examples show, acids (in the colloquial sense) can be solutions or pure substances, and can be derived from acids (in the strict sense) that are solids, liquids, or gases. Strong acids\n",
      ". For them, the publication of Charles Darwin's On the Origin of Species was the epiphany of everything they had begun to suspect. Darwin himself arrived at his conclusions through comparison of species he had seen in agronomy and in the wild.\n",
      "\n",
      "Darwin and Wallace unveiled evolution in the late 1850s. There was an immediate rush to bring it into the social sciences. Paul Broca in Paris was in the process of breaking away from the Société de biologie to form the first of the explicitly anthropological societies, the Société d'Anthropologie de Paris, meeting for the first time in Paris\n",
      " communication. Languages that communicate efficiently will avoid sending information that is redundant with information provided in the context. This can be shown mathematically to result in a system which is ambiguous when context is neglected. In this way, ambiguity is viewed as a generally useful feature of a linguistic system. \n",
      "\n",
      "Linguistic ambiguity can be a problem in law, because the interpretation of written documents and oral agreements is often of paramount importance.\n",
      "\n",
      "Lexical ambiguity \n",
      "The lexical ambiguity of a word or phrase applies to it having more than one meaning in the language to which the word belongs. \"Meaning\" here refers to whatever should be\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for ex in train_dataloader:\n",
    "    # print(ex)\n",
    "    input_ids = ex['input_ids']\n",
    "    # print(input_ids)\n",
    "    print(tokenizer.decode(ex['input_ids'].tolist()[0]))\n",
    "    i += 1\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|endoftext|>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<|endoftext|>'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'�'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7454, 2402,  257,  640,   11]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Once upon a time,\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kanye West\n"
     ]
    }
   ],
   "source": [
    "context = tokenizer.encode(\"Kanye West\", return_tensors=\"pt\")\n",
    "print(tokenizer.decode(context[0]))  # Check what tokens are being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
