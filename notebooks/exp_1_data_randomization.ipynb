{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Assuming the text_prediction module is in the 'src' directory\n",
    "module_path = \"/Users/aadil/Documents/Workspace/ml-projects/wiki-llm/src/\"  # Replace with the correct path to your 'src' directory\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, DistributedSampler, Dataset as TDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "import os\n",
    "import pyarrow as pa\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "import logging\n",
    "import random\n",
    "\n",
    "from text_prediction.utils import RankFilter\n",
    "\n",
    "class TokenizedDataset(TorchDataset):\n",
    "    def __init__(self, tokenized_dataset, block_size, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        tokenized_examples: List of tokenized sequences (each a list of token IDs).\n",
    "        block_size: Length of each input sequence.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self._data = [torch.tensor(example['input_ids'], dtype=torch.long) for example in tokenized_dataset if len(example['input_ids']) > block_size]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._data) - self.block_size  # Max index to sample from\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return a single sample (input and target sequences)\n",
    "        \"\"\"\n",
    "        example = self._data[idx]\n",
    "        ix = torch.randint(0, len(example) - self.block_size, (1,)).item()\n",
    "        input_seq = example[ix:ix + self.block_size]\n",
    "        label_seq = example[ix + 1:ix + self.block_size + 1]\n",
    "        return input_seq.to(self.device), label_seq.to(self.device)\n",
    "\n",
    "class DataPipeline:\n",
    "\n",
    "    def __init__(self, tokenizer, max_len, block_size, regenerate=False, num_samples=10000, verbose=False, augment_data=False, parent_path=\".\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.block_size = block_size\n",
    "        self.regenerate = regenerate\n",
    "        self.num_samples = num_samples\n",
    "        self.train_dataloader = None\n",
    "        self.val_dataloader = None\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.verbose = verbose\n",
    "        self.augment_data = augment_data\n",
    "        self.parent_path = parent_path\n",
    "\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')  # Update logging format\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.addFilter(RankFilter(0))\n",
    "        self.tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<ARTICLE_START>\", \"<ARTICLE_END>\"]})\n",
    "\n",
    "    def log(self, message, level=logging.INFO):\n",
    "        if self.verbose:\n",
    "            self.logger.log(level, message)\n",
    "\n",
    "    def _tokenize_function(self, examples):\n",
    "        \"\"\"Tokenizes the examples, adds special tokens, and optionally augments data.\"\"\"\n",
    "        \n",
    "        # Add special tokens to the beginning and end of each text\n",
    "        texts_with_special_tokens = [\n",
    "            \"<ARTICLE_START>\" + text + \"<ARTICLE_END>\" for text in examples[\"text\"]\n",
    "        ]\n",
    "\n",
    "        tokenized = self.tokenizer(\n",
    "            texts_with_special_tokens,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "\n",
    "        if self.augment_data:\n",
    "            # Add data augmentation logic here\n",
    "            tokenized[\"input_ids\"] = self._augment(tokenized[\"input_ids\"])\n",
    "\n",
    "        return tokenized\n",
    "\n",
    "    def _augment(self, input_ids):\n",
    "        # Implement data augmentation logic\n",
    "        # For example, randomly mask some tokens\n",
    "        augmented = []\n",
    "        for ids in input_ids:\n",
    "            if random.random() < 0.1:  # 10% chance to mask a token\n",
    "                ids[random.randint(0, len(ids) - 1)] = self.tokenizer.mask_token_id\n",
    "            augmented.append(ids)\n",
    "        return augmented\n",
    "\n",
    "    def _get_tokenized_dataset(self, split=\"train\"):\n",
    "        tokenized_dataset_path = f\"{self.parent_path}/data/{self.tokenizer.name_or_path}/wiki/{split}/tokenized_augmented\" if self.augment_data else f\"{self.parent_path}/data/{self.tokenizer.name_or_path}/wiki/{split}/tokenized\"\n",
    "\n",
    "        if not os.path.exists(tokenized_dataset_path) or self.regenerate:\n",
    "            self.log(\"Local cache of dataset not found, downloading and tokenizing dataset...\")\n",
    "            # Load dataset (small subset of num_samples samples)\n",
    "            ds = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=split)\n",
    "            ds = ds.select(range(self.num_samples))\n",
    "            # Select only the 'text' column\n",
    "            ds = ds.remove_columns([col for col in ds.column_names if col != \"text\"])\n",
    "            # Tokenize the dataset\n",
    "            ds = ds.map(self._tokenize_function, batched=True)\n",
    "            ds.save_to_disk(tokenized_dataset_path)\n",
    "        else:\n",
    "            self.log(\"Local cache of dataset found, loading tokenized dataset...\")\n",
    "            ds = load_from_disk(tokenized_dataset_path)\n",
    "        return ds\n",
    "\n",
    "    def _is_dataset_valid(self, dataset_path):\n",
    "        try:\n",
    "            ds = load_from_disk(dataset_path)\n",
    "            # Attempt to read a small portion of the dataset to ensure it's valid\n",
    "            _ = ds[:1]\n",
    "            return True\n",
    "        except (pa.lib.ArrowInvalid, FileNotFoundError):\n",
    "            return False\n",
    "\n",
    "    @staticmethod\n",
    "    def custom_collate(batch, tokenizer):\n",
    "        \"\"\"\n",
    "        Custom collate function to stack sequences into a batch.\n",
    "        \"\"\"\n",
    "        inputs, labels = zip(*batch)\n",
    "        \n",
    "        # Ensure tokenizer has a valid pad token\n",
    "        pad_token_id = tokenizer.pad_token_id\n",
    "        if pad_token_id is None:\n",
    "            pad_token_id = tokenizer.eos_token_id  # Use EOS token as a fallback\n",
    "        \n",
    "        inputs = pad_sequence(inputs, batch_first=True, padding_value=pad_token_id)\n",
    "        labels = pad_sequence(labels, batch_first=True, padding_value=pad_token_id)    \n",
    "        return inputs, labels\n",
    "\n",
    "    def get_dataloader(self, batch_size, shuffle=True):\n",
    "        # DataLoader with random sampling\n",
    "        tds = self._get_tokenized_dataset()\n",
    "        dataset = TokenizedDataset(tds, self.block_size, self.device)\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            collate_fn=lambda batch: DataPipeline.custom_collate(batch, self.tokenizer),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "dp = DataPipeline(tokenizer, max_len=512, block_size=128, regenerate=False, num_samples=10000, verbose=True, augment_data=False, parent_path=\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 12:16:07,997 - Local cache of dataset found, loading tokenized dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example 0 inputs\n",
      "The common law offence of affray was abolished for England and Wales on 1 April 1987. Affray is now a statutory offence that is triable either way. It is created by section 3 of the Public Order Act 1986 which provides:\n",
      "\n",
      "The term \"violence\" is defined by section 8.\n",
      "\n",
      "Section 3(6) once provided that a constable could arrest without warrant anyone he reasonably suspected to be committing affray, but that subsection was repealed by paragraph 26(2) of Schedule 7 to, and Schedule 17 to, the Serious Organised Crime and Police Act 2005, which includes more general provisions for police to make arrests\n",
      "example 0 labels\n",
      " common law offence of affray was abolished for England and Wales on 1 April 1987. Affray is now a statutory offence that is triable either way. It is created by section 3 of the Public Order Act 1986 which provides:\n",
      "\n",
      "The term \"violence\" is defined by section 8.\n",
      "\n",
      "Section 3(6) once provided that a constable could arrest without warrant anyone he reasonably suspected to be committing affray, but that subsection was repealed by paragraph 26(2) of Schedule 7 to, and Schedule 17 to, the Serious Organised Crime and Police Act 2005, which includes more general provisions for police to make arrests without\n",
      "example 1 inputs\n",
      " line. It was originally a command-line based system. The command-line processing abilities are now available through the GUI of the ArcGIS Desktop product.\n",
      "\n",
      "History\n",
      "\n",
      "ARC/INFO\n",
      "ESRI launched the first version of ARC/INFO - which it claims as \"the very first modern GIS\" - in 1982 on minicomputers. The name refers to its architecture as a geographic information system composed of:\n",
      " geographic input, processing, and output tools (\"ARC\") with \n",
      " a complementary, but separate database (\"INFO\")\n",
      "\n",
      "The early releases of ARC/INFO comprised a set of FORTRAN programs\n",
      "example 1 labels\n",
      ". It was originally a command-line based system. The command-line processing abilities are now available through the GUI of the ArcGIS Desktop product.\n",
      "\n",
      "History\n",
      "\n",
      "ARC/INFO\n",
      "ESRI launched the first version of ARC/INFO - which it claims as \"the very first modern GIS\" - in 1982 on minicomputers. The name refers to its architecture as a geographic information system composed of:\n",
      " geographic input, processing, and output tools (\"ARC\") with \n",
      " a complementary, but separate database (\"INFO\")\n",
      "\n",
      "The early releases of ARC/INFO comprised a set of FORTRAN programs linked\n",
      "example 2 inputs\n",
      "CJEL-FM is a Canadian radio station being licensed to Winkler, Manitoba, and serving the Pembina Valley region of Manitoba broadcasting at 93.5 FM. The station airs a hot adult contemporary format branded on-air as The Eagle 93.5 and is owned by Golden West Broadcasting.\n",
      "\n",
      "History\n",
      "On October 19, 1999, Golden West Broadcasting Ltd. received approval by the Canadian Radio-television and Telecommunications Commission (CRTC) for a new English-language FM radio station programming undertaking at Winkler. The new station would broadcast on the frequency 93.5 MHz with an effective radiated power of 100\n",
      "example 2 labels\n",
      "JEL-FM is a Canadian radio station being licensed to Winkler, Manitoba, and serving the Pembina Valley region of Manitoba broadcasting at 93.5 FM. The station airs a hot adult contemporary format branded on-air as The Eagle 93.5 and is owned by Golden West Broadcasting.\n",
      "\n",
      "History\n",
      "On October 19, 1999, Golden West Broadcasting Ltd. received approval by the Canadian Radio-television and Telecommunications Commission (CRTC) for a new English-language FM radio station programming undertaking at Winkler. The new station would broadcast on the frequency 93.5 MHz with an effective radiated power of 100,\n",
      "example 3 inputs\n",
      "\n",
      "Listed below are the head U.S. diplomatic agents to East Germany, their diplomatic rank, and the effective start and end of their service in East Germany.\n",
      "\n",
      "List of United States ambassadors to East Germany\n",
      "\n",
      "Heads of the U.S. Embassy at East Berlin (1974–1990)\n",
      "\n",
      "See also\n",
      "\n",
      " East Germany–United States relations\n",
      " Embassy of the United States, Berlin\n",
      " Embassy of Germany, Washington, D.C.\n",
      " Ambassadors of East Germany to the United States\n",
      " Germany–United States relations\n",
      " Ambassadors of Germany to the United States\n",
      " Ambassadors of the United States to Germany\n",
      "\n",
      "example 3 labels\n",
      "Listed below are the head U.S. diplomatic agents to East Germany, their diplomatic rank, and the effective start and end of their service in East Germany.\n",
      "\n",
      "List of United States ambassadors to East Germany\n",
      "\n",
      "Heads of the U.S. Embassy at East Berlin (1974–1990)\n",
      "\n",
      "See also\n",
      "\n",
      " East Germany–United States relations\n",
      " Embassy of the United States, Berlin\n",
      " Embassy of Germany, Washington, D.C.\n",
      " Ambassadors of East Germany to the United States\n",
      " Germany–United States relations\n",
      " Ambassadors of Germany to the United States\n",
      " Ambassadors of the United States to Germany\n",
      "\n",
      "\n",
      "example 4 inputs\n",
      " lines between the state of Georgia and Creek Indian lands as part of the Treaty of the Creek Agency (1818).\n",
      "\n",
      "Nearly a decade later, Lumpkin returned to Congress, being elected to the Twentieth, Twenty-first, and Twenty-second Congresses and serving from March 4, 1827, until his resignation in 1831 before the convening of the Twenty-second Congress. He ran for the governorship; he was also an appointed commissioner on the Georgia–Florida boundary line commission.\n",
      "\n",
      "Lumpkin was elected Governor of Georgia in November 1831, for what was then the standard two-year term.\n",
      "example 4 labels\n",
      " between the state of Georgia and Creek Indian lands as part of the Treaty of the Creek Agency (1818).\n",
      "\n",
      "Nearly a decade later, Lumpkin returned to Congress, being elected to the Twentieth, Twenty-first, and Twenty-second Congresses and serving from March 4, 1827, until his resignation in 1831 before the convening of the Twenty-second Congress. He ran for the governorship; he was also an appointed commissioner on the Georgia–Florida boundary line commission.\n",
      "\n",
      "Lumpkin was elected Governor of Georgia in November 1831, for what was then the standard two-year term. In\n",
      "example 5 inputs\n",
      " be governed by regulation, contract, technical specifications or other published standards. Engineering judgment may include such factors as:\n",
      " The precision and accuracy of measurement and the associated uncertainty of measurement.\n",
      " The statistical confidence interval or tolerance interval of the initial measurement.\n",
      " The number of significant figures of the measurement.\n",
      " The intended use of the measurement including the engineering tolerances.\n",
      " Historical definitions of the units and their derivatives used in old measurements; e.g., international foot vs. US survey foot.\n",
      "\n",
      "Some conversions from one system of units to another need to be exact, without increasing or decreasing the precision of the first measurement. This is sometimes\n",
      "example 5 labels\n",
      " governed by regulation, contract, technical specifications or other published standards. Engineering judgment may include such factors as:\n",
      " The precision and accuracy of measurement and the associated uncertainty of measurement.\n",
      " The statistical confidence interval or tolerance interval of the initial measurement.\n",
      " The number of significant figures of the measurement.\n",
      " The intended use of the measurement including the engineering tolerances.\n",
      " Historical definitions of the units and their derivatives used in old measurements; e.g., international foot vs. US survey foot.\n",
      "\n",
      "Some conversions from one system of units to another need to be exact, without increasing or decreasing the precision of the first measurement. This is sometimes called\n"
     ]
    }
   ],
   "source": [
    "train_loader = dp.get_dataloader(32)\n",
    "\n",
    "# Get a batch\n",
    "i = 0\n",
    "for x, y in train_loader:\n",
    "    print(f\"example {i} inputs\")\n",
    "    print(tokenizer.decode(x[0]))\n",
    "    print(f\"example {i} labels\")\n",
    "    print(tokenizer.decode(y[0]))\n",
    "    i += 1\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'text_prediction'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtext_prediction\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'text_prediction'"
     ]
    }
   ],
   "source": [
    "import text_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: text_prediction in /Users/aadil/Documents/Workspace/ml-projects/wiki-llm/.venv/lib/python3.13/site-packages (0.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --no-cache-dir text_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Assuming the text_prediction module is in the 'src' directory\n",
    "module_path = \"/Users/aadil/Documents/Workspace/ml-projects/wiki-llm/src/\"  # Replace with the correct path to your 'src' directory\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aadil/Documents/Workspace/ml-projects/wiki-llm/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from text_prediction.data_pipeline import DataPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "dp = DataPipeline(\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=512,\n",
    "    block_size=128,\n",
    "    num_samples=10000,\n",
    "    verbose=True,\n",
    "    augment_data=False,\n",
    "    parent_path=\"..\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 23:14:19,030 - Local cache of dataset found, loading tokenized dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tokenized dataset\n"
     ]
    }
   ],
   "source": [
    "dl = dp.get_dataloader(batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Input shape: torch.Size([32, 128])\n",
      "Label shape: torch.Size([32, 128])\n",
      "Input sequence: : Army, Navy and Aerospace Force (Title VII, chapter VII, Art. 217)\n",
      "\n",
      "This is a subtle yet important distinction, both in terms of emphasizing the civil nature of the National Police, but also adapting the national police to function as a paramilitary force which can perform military duties as a result of the Colombian Conflict. This has led to some of the most important police units adopting military training and conducting special operations alongside the Colombian Army, Aerospace Force, and Navy. ThereforeThe history of Colombia includes the settlements and society by indigenous peoples, most notably, the Muisca Confederation, Quimbaya Civilization, and Tairona Chief\n",
      "Output sequence:  Army, Navy and Aerospace Force (Title VII, chapter VII, Art. 217)\n",
      "\n",
      "This is a subtle yet important distinction, both in terms of emphasizing the civil nature of the National Police, but also adapting the national police to function as a paramilitary force which can perform military duties as a result of the Colombian Conflict. This has led to some of the most important police units adopting military training and conducting special operations alongside the Colombian Army, Aerospace Force, and Navy. ThereforeThe history of Colombia includes the settlements and society by indigenous peoples, most notably, the Muisca Confederation, Quimbaya Civilization, and Tairona Chiefdoms\n",
      "Batch 2\n",
      "Input shape: torch.Size([32, 128])\n",
      "Label shape: torch.Size([32, 128])\n",
      "Input sequence: KPSZ is powered at 10,000 watts by day and 5,000 watts at night.  It uses a directional antenna with a six-tower array to protect other stations on 940 AM, a Canadian and Mexican clear channel frequency.  The transmitter is on 228th Avenue at Carpenter Street in Hartford.\n",
      "\n",
      "History\n",
      "\n",
      "KIOA: 1948–1957\n",
      "On January 15, 1948, the station signed on as KIOA. Its studios were located in the Onthank building at 10th and Mulberry in downtown Des Moines. The transmitter and towers are located two miles northeast of Hartford, Iowa. The owners\n",
      "Output sequence: PSZ is powered at 10,000 watts by day and 5,000 watts at night.  It uses a directional antenna with a six-tower array to protect other stations on 940 AM, a Canadian and Mexican clear channel frequency.  The transmitter is on 228th Avenue at Carpenter Street in Hartford.\n",
      "\n",
      "History\n",
      "\n",
      "KIOA: 1948–1957\n",
      "On January 15, 1948, the station signed on as KIOA. Its studios were located in the Onthank building at 10th and Mulberry in downtown Des Moines. The transmitter and towers are located two miles northeast of Hartford, Iowa. The owners of\n",
      "Batch 3\n",
      "Input shape: torch.Size([32, 128])\n",
      "Label shape: torch.Size([32, 128])\n",
      "Input sequence: ov finished first in his first international tournament, in Třinec, severalThe aspect ratio of a geometric shape is the ratio of its sizes in different dimensions. For example, the aspect ratio of a rectangle is the ratio of its longer side to its shorter side—the ratio of width to height, when the rectangle is oriented as a \"landscape\".\n",
      "\n",
      "The aspect ratio is most often expressed as two integer numbers separated by a colon (x:y), less commonly as a simple or decimal fraction. The values x and y do not represent actual widths and heights but, rather, the proportion between width and height.\n",
      "Output sequence:  finished first in his first international tournament, in Třinec, severalThe aspect ratio of a geometric shape is the ratio of its sizes in different dimensions. For example, the aspect ratio of a rectangle is the ratio of its longer side to its shorter side—the ratio of width to height, when the rectangle is oriented as a \"landscape\".\n",
      "\n",
      "The aspect ratio is most often expressed as two integer numbers separated by a colon (x:y), less commonly as a simple or decimal fraction. The values x and y do not represent actual widths and heights but, rather, the proportion between width and height. As\n",
      "Batch 4\n",
      "Input shape: torch.Size([32, 128])\n",
      "Label shape: torch.Size([32, 128])\n",
      "Input sequence:  provisions. Messengers came to O'Donnell from the Scots, whom he had before invited over, to inform him that they had arrived at Derry. And those who had come thither were Donnell Gorm Mac Donnell, and Mac Leod of Ara. O'Donnell then set out with a small number of his forces to hire them; and he left another large party of them with Maguire to assist him, and he ordered them to remain blockading the castle. When the Lord Justice, Sir William Fitzwilliam, had received intelligence that the warders of Enniskillen were in want of stores and\n",
      "Output sequence: . Messengers came to O'Donnell from the Scots, whom he had before invited over, to inform him that they had arrived at Derry. And those who had come thither were Donnell Gorm Mac Donnell, and Mac Leod of Ara. O'Donnell then set out with a small number of his forces to hire them; and he left another large party of them with Maguire to assist him, and he ordered them to remain blockading the castle. When the Lord Justice, Sir William Fitzwilliam, had received intelligence that the warders of Enniskillen were in want of stores andB\n",
      "Batch 5\n",
      "Input shape: torch.Size([32, 128])\n",
      "Label shape: torch.Size([32, 128])\n",
      "Input sequence:  an attack, he was invalided out of the war, and his football career looked in doubt. However, he managed to recover fully and returned to Luton Town after the end of the war, making himself known as a winger who could play on either flank. Hoar played over 150 league matches for Luton between 1919 and 1924, as they played in the Southern League and later the Third Division South.\n",
      "\n",
      "In late 1924, Hoar joined Arsenal for £3,000, making his debut against Cardiff City on 29 November 1924, and went on to make nineteen appearances that season; he also had trials with England but never made\n",
      "Output sequence:  attack, he was invalided out of the war, and his football career looked in doubt. However, he managed to recover fully and returned to Luton Town after the end of the war, making himself known as a winger who could play on either flank. Hoar played over 150 league matches for Luton between 1919 and 1924, as they played in the Southern League and later the Third Division South.\n",
      "\n",
      "In late 1924, Hoar joined Arsenal for £3,000, making his debut against Cardiff City on 29 November 1924, and went on to make nineteen appearances that season; he also had trials with England but never made it\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    x, y = next(iter(dl))\n",
    "    print(f\"Batch {i + 1}\")\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Label shape:\", y.shape)\n",
    "    print(f\"Input sequence: {tokenizer.decode(x[0].tolist())}\")\n",
    "    print(f\"Output sequence: {tokenizer.decode(y[0].tolist())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
