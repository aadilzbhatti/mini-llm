{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Assuming the text_prediction module is in the 'src' directory\n",
    "module_path = \"/Users/aadil/Documents/Workspace/ml-projects/wiki-llm/src/\"  # Replace with the correct path to your 'src' directory\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, DistributedSampler, Dataset as TDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "import os\n",
    "import pyarrow as pa\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "\n",
    "from text_prediction.utils import RankFilter\n",
    "\n",
    "class TokenizedDataset(TorchDataset):\n",
    "    def __init__(self, tokenized_dataset, block_size, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        tokenized_examples: List of tokenized sequences (each a list of token IDs).\n",
    "        block_size: Length of each input sequence.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self._data = [torch.tensor(example['input_ids'], dtype=torch.long) for example in tokenized_dataset if len(example['input_ids']) > block_size]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._data) - self.block_size  # Max index to sample from\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return a single sample (input and target sequences)\n",
    "        \"\"\"\n",
    "        example = self._data[idx]\n",
    "        ix = torch.randint(0, len(example) - self.block_size, (1,)).item()\n",
    "        input_seq = example[ix:ix + self.block_size]\n",
    "        label_seq = example[ix + 1:ix + self.block_size + 1]\n",
    "        return input_seq.to(self.device), label_seq.to(self.device)\n",
    "\n",
    "class DataPipeline:\n",
    "\n",
    "    def __init__(self, tokenizer, max_len, block_size, regenerate=False, num_samples=10000, verbose=False, augment_data=False, parent_path=\".\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.block_size = block_size\n",
    "        self.regenerate = regenerate\n",
    "        self.num_samples = num_samples\n",
    "        self.train_dataloader = None\n",
    "        self.val_dataloader = None\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.verbose = verbose\n",
    "        self.augment_data = augment_data\n",
    "        self.parent_path = parent_path\n",
    "\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')  # Update logging format\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.addFilter(RankFilter(0))\n",
    "        self.tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<ARTICLE_START>\", \"<ARTICLE_END>\"]})\n",
    "\n",
    "    def log(self, message, level=logging.INFO):\n",
    "        if self.verbose:\n",
    "            self.logger.log(level, message)\n",
    "\n",
    "    def _sanitize_text(self, text):\n",
    "        \"\"\"Sanitizes the text while preserving meaningful dashes, numerical values with units, and hyperlink text.\"\"\"\n",
    "        \n",
    "        # Preserve Markdown-style and HTML links by keeping only the text\n",
    "        text = re.sub(r'\\[([^\\]]+)\\]\\(http\\S+\\)', r'\\1', text)  # Handles Markdown links\n",
    "        text = re.sub(r'<a\\s+href=[\"\\']http\\S+[\"\\']>(.*?)</a>', r'\\1', text, flags=re.IGNORECASE)  # Handles HTML links\n",
    "\n",
    "        # Remove standalone URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "        # Ensure we preserve dashes in hyphenated place names (e.g., Indianapolis–Carmel–Anderson)\n",
    "        text = re.sub(r'(\\w)\\s*[-–]\\s*(\\w)', r'\\1–\\2', text)  # Normalize hyphens and remove unwanted spaces around them\n",
    "\n",
    "        # Keep valid numerical values with units (e.g., \"3.0 square miles (7.8 km2)\")\n",
    "        text = re.sub(r'(\\d+(\\.\\d+)?)\\s*([a-zA-Z²]+)', r'\\1 \\3', text)  # Ensures numbers and units stay together\n",
    "        text = re.sub(r'\\((\\d+(\\.\\d+)?\\s*[a-zA-Z²]+)\\)', r'(\\1)', text)  # Ensures parenthetical units remain intact\n",
    "\n",
    "        # Preserve valid year ranges (e.g., 1992-2002)\n",
    "        text = re.sub(r'(?<!\\d)(\\d{4})-(\\d{4})(?!\\d)', r'\\1-\\2', text)  # Ensure valid formatting\n",
    "\n",
    "        # Remove unwanted characters but keep punctuation, parentheses, and percentage signs\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s.,!?\\'\\\"()%-²]', '', text)\n",
    "\n",
    "        # Normalize spaces\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "    def _tokenize_function(self, examples):\n",
    "        \"\"\"Tokenizes the examples, adds special tokens, and optionally augments data.\"\"\"\n",
    "        \n",
    "        # Sanitize text data\n",
    "        sanitized_texts = [self._sanitize_text(text) for text in examples[\"text\"]]\n",
    "\n",
    "        # Add special tokens to the beginning and end of each text\n",
    "        texts_with_special_tokens = [\n",
    "            \"<ARTICLE_START>\" + text + \"<ARTICLE_END>\" for text in sanitized_texts\n",
    "        ]\n",
    "\n",
    "        tokenized = self.tokenizer(\n",
    "            texts_with_special_tokens,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "\n",
    "        if self.augment_data:\n",
    "            # Add data augmentation logic here\n",
    "            tokenized[\"input_ids\"] = self._augment(tokenized[\"input_ids\"])\n",
    "\n",
    "        return tokenized\n",
    "\n",
    "    def _augment(self, input_ids):\n",
    "        # Implement data augmentation logic\n",
    "        # For example, randomly mask some tokens\n",
    "        augmented = []\n",
    "        for ids in input_ids:\n",
    "            if random.random() < 0.1:  # 10% chance to mask a token\n",
    "                ids[random.randint(0, len(ids) - 1)] = self.tokenizer.mask_token_id\n",
    "            augmented.append(ids)\n",
    "        return augmented\n",
    "\n",
    "    def _get_tokenized_dataset(self):\n",
    "        tokenized_dataset_path = f\"{self.parent_path}/data/{self.tokenizer.name_or_path}/wiki/tokenized_augmented\" if self.augment_data else f\"{self.parent_path}/data/{self.tokenizer.name_or_path}/wiki/tokenized\"\n",
    "\n",
    "        if not os.path.exists(tokenized_dataset_path) or self.regenerate:\n",
    "            self.log(\"Local cache of dataset not found, downloading and tokenizing dataset...\")\n",
    "            # Load dataset (small subset of num_samples samples)\n",
    "            ds = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=\"train\")\n",
    "            ds = ds.select(range(self.num_samples))\n",
    "            # Select only the 'text' column\n",
    "            ds = ds.remove_columns([col for col in ds.column_names if col != \"text\"])\n",
    "            # Tokenize the dataset\n",
    "            ds = ds.map(self._tokenize_function, batched=True)\n",
    "            ds.save_to_disk(tokenized_dataset_path)\n",
    "        else:\n",
    "            self.log(\"Local cache of dataset found, loading tokenized dataset...\")\n",
    "            ds = load_from_disk(tokenized_dataset_path)\n",
    "        return ds\n",
    "\n",
    "    def _is_dataset_valid(self, dataset_path):\n",
    "        try:\n",
    "            ds = load_from_disk(dataset_path)\n",
    "            # Attempt to read a small portion of the dataset to ensure it's valid\n",
    "            _ = ds[:1]\n",
    "            return True\n",
    "        except (pa.lib.ArrowInvalid, FileNotFoundError):\n",
    "            return False\n",
    "\n",
    "    @staticmethod\n",
    "    def custom_collate(batch, tokenizer):\n",
    "        inputs, labels = zip(*batch)\n",
    "        pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "        inputs = pad_sequence(inputs, batch_first=True, padding_value=pad_token_id)\n",
    "        labels = pad_sequence(labels, batch_first=True, padding_value=pad_token_id)\n",
    "\n",
    "        # Create attention masks\n",
    "        B, T = inputs.shape\n",
    "        tril_mask = torch.tril(torch.ones(T, T)).bool().to(inputs.device) #move tril to the same device as inputs\n",
    "        attention_mask = (inputs != pad_token_id).unsqueeze(1).repeat(1, T, 1) & tril_mask #create a causal mask.\n",
    "        return inputs, labels, attention_mask.to(inputs.device)\n",
    "\n",
    "    def get_dataloader(self, batch_size, split=\"train\", shuffle=True):\n",
    "        # Ensure split is either \"train\" or \"val\"\n",
    "        if split not in [\"train\", \"val\"]:\n",
    "            raise ValueError(\"split must be either 'train' or 'val'\")\n",
    "        \n",
    "        # DataLoader with random sampling\n",
    "        tds = self._get_tokenized_dataset()\n",
    "        \n",
    "        # Split the dataset into train and validation sets\n",
    "        split_datasets = tds.train_test_split(test_size=0.2, seed=42)\n",
    "        selected_dataset = split_datasets[\"train\" if split == \"train\" else \"test\"]\n",
    "        \n",
    "        dataset = TokenizedDataset(selected_dataset, self.block_size, self.device)\n",
    "        \n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            collate_fn=lambda batch: DataPipeline.custom_collate(batch, self.tokenizer),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "dp = DataPipeline(tokenizer, max_len=512, block_size=128, regenerate=False, num_samples=10000, verbose=True, augment_data=False, parent_path=\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 08:33:11,571 - Local cache of dataset not found, downloading and tokenizing dataset...\n",
      "Map: 100%|██████████| 10000/10000 [00:23<00:00, 419.99 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 46121.41 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example 0 inputs\n",
      " played eight seasons for the Denver Broncos from 1996 through 2003 in the National Football League (NFL). He is the cousin of former NBA player Cuttino Mobley. Biography One of nine children born to parents who divorced when he was thirteen years old, Mobley lived with his father until the age of sixteen. After his father suffered a stroke, Mobley moved in with his mother, who demanded he leave high school to help support his family. Mobley spent a year living on the street in an old car before a friend's family took him in. Lacking the academic credentials for a Division I school, Mobley went on to\n",
      "example 0 labels\n",
      " eight seasons for the Denver Broncos from 1996 through 2003 in the National Football League (NFL). He is the cousin of former NBA player Cuttino Mobley. Biography One of nine children born to parents who divorced when he was thirteen years old, Mobley lived with his father until the age of sixteen. After his father suffered a stroke, Mobley moved in with his mother, who demanded he leave high school to help support his family. Mobley spent a year living on the street in an old car before a friend's family took him in. Lacking the academic credentials for a Division I school, Mobley went on to play\n",
      "example 1 inputs\n",
      " appears to be the Punic word for \"servant\" or \"slave\"; the second element seems shared by the Phoenician names for Gadir (now Cadiz) and Cythera but of unclear meaning. It appears in Greek sources as t bdra () and Adra (), bdara (), and t bdron (). History Abdera was founded in the 8 th century BCE as a Phoenician colony. It became a Carthaginian trading station and, after a period of decline, became one of the more important towns in the Roman province of Hispania Baetica. Tiberius seems to have\n",
      "example 1 labels\n",
      " to be the Punic word for \"servant\" or \"slave\"; the second element seems shared by the Phoenician names for Gadir (now Cadiz) and Cythera but of unclear meaning. It appears in Greek sources as t bdra () and Adra (), bdara (), and t bdron (). History Abdera was founded in the 8 th century BCE as a Phoenician colony. It became a Carthaginian trading station and, after a period of decline, became one of the more important towns in the Roman province of Hispania Baetica. Tiberius seems to have made\n",
      "example 2 inputs\n",
      " the recipe for Drambuie liqueur was given by Bonnie Prince Charlie to Clan MacKinnon who then passed it onto James Ross late 19 th century. Ross ran the Broadford Inn (now the Broadford Hotel), where he developed and improved the recipe, initially for his friends and then later to patrons. Ross then began to sell it further afield and the name was registered as a trademark in 1893. Throughout the 1800 s, marble was extracted from a quarry at the foot of Beinn na Caillich (Hill of the Old Woman) and in 1904 the Skye Marble Railway was built to carry the marble to the new\n",
      "example 2 labels\n",
      " recipe for Drambuie liqueur was given by Bonnie Prince Charlie to Clan MacKinnon who then passed it onto James Ross late 19 th century. Ross ran the Broadford Inn (now the Broadford Hotel), where he developed and improved the recipe, initially for his friends and then later to patrons. Ross then began to sell it further afield and the name was registered as a trademark in 1893. Throughout the 1800 s, marble was extracted from a quarry at the foot of Beinn na Caillich (Hill of the Old Woman) and in 1904 the Skye Marble Railway was built to carry the marble to the new pier\n",
      "example 3 inputs\n",
      "<ARTICLE_START>\"Ukridge's Dog College\" is a short story by P. G. Wodehouse, which first appeared in the United States in the April 1923 issue of Cosmopolitan, and in the United Kingdom in the May 1923 Strand. It features the irrepressible Stanley Featherstonehaugh Ukridge and was included in the collection Ukridge, published in 1924. Plot Ukridge is introduced to the reader as a childhood friend of the narrator, later revealed to be James \"Corky\" Corcoran, who having been expelled from school for sneaking out of the school grounds to attend a fair has travelled the world undertaking\n",
      "example 3 labels\n",
      "\"Ukridge's Dog College\" is a short story by P. G. Wodehouse, which first appeared in the United States in the April 1923 issue of Cosmopolitan, and in the United Kingdom in the May 1923 Strand. It features the irrepressible Stanley Featherstonehaugh Ukridge and was included in the collection Ukridge, published in 1924. Plot Ukridge is introduced to the reader as a childhood friend of the narrator, later revealed to be James \"Corky\" Corcoran, who having been expelled from school for sneaking out of the school grounds to attend a fair has travelled the world undertaking all\n",
      "example 4 inputs\n",
      " of two or more phases) or intermetallic. An alloy may be a solid solution of metal elements (a single phase, where all metallic grains (crystals) are of the same composition) or a mixture of metallic phases (two or more solutions, forming a microstructure of different crystals within the metal). Examples of alloys include red gold (gold and copper), white gold (gold and silver), sterling silver (silver and copper), steel or silicon steel (iron with nonmetallic carbon or silicon respectively), solder, brass, pewter, duralumin, bronze, and amalgams. Alloys are used in\n",
      "example 4 labels\n",
      " two or more phases) or intermetallic. An alloy may be a solid solution of metal elements (a single phase, where all metallic grains (crystals) are of the same composition) or a mixture of metallic phases (two or more solutions, forming a microstructure of different crystals within the metal). Examples of alloys include red gold (gold and copper), white gold (gold and silver), sterling silver (silver and copper), steel or silicon steel (iron with nonmetallic carbon or silicon respectively), solder, brass, pewter, duralumin, bronze, and amalgams. Alloys are used in a\n",
      "example 5 inputs\n",
      " urban in character. It is bordered to the east by Cardinham parish, to the southeast by Lanhydrock parish, to the southwest and west by Lanivet parish, and to the north by Helland parish. Bodmin had a population of 14,736 as of the 2011 Census. It was formerly the county town of Cornwall until the Crown Courts moved to Truro which is also the administrative centre (before 1835 the county town was Launceston). Bodmin was in the administrative North Cornwall District until local government reorganisation in 2009 abolished the District (see also Cornwall Council). The town is part of the North Cornwall\n",
      "example 5 labels\n",
      " in character. It is bordered to the east by Cardinham parish, to the southeast by Lanhydrock parish, to the southwest and west by Lanivet parish, and to the north by Helland parish. Bodmin had a population of 14,736 as of the 2011 Census. It was formerly the county town of Cornwall until the Crown Courts moved to Truro which is also the administrative centre (before 1835 the county town was Launceston). Bodmin was in the administrative North Cornwall District until local government reorganisation in 2009 abolished the District (see also Cornwall Council). The town is part of the North Cornwall parliamentary\n"
     ]
    }
   ],
   "source": [
    "train_loader = dp.get_dataloader(32)\n",
    "\n",
    "# Get a batch\n",
    "i = 0\n",
    "for x, y, _ in train_loader:\n",
    "    print(f\"example {i} inputs\")\n",
    "\n",
    "    print(tokenizer.decode(x[0]))\n",
    "    print(f\"example {i} labels\")\n",
    "    print(tokenizer.decode(y[0]))\n",
    "    i += 1\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'text_prediction'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtext_prediction\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'text_prediction'"
     ]
    }
   ],
   "source": [
    "import text_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: text_prediction in /Users/aadil/Documents/Workspace/ml-projects/wiki-llm/.venv/lib/python3.13/site-packages (0.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --no-cache-dir text_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Assuming the text_prediction module is in the 'src' directory\n",
    "module_path = \"/Users/aadil/Documents/Workspace/ml-projects/wiki-llm/src/\"  # Replace with the correct path to your 'src' directory\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aadil/Documents/Workspace/ml-projects/wiki-llm/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from text_prediction.data_pipeline import DataPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "dp = DataPipeline(\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=512,\n",
    "    block_size=128,\n",
    "    num_samples=10000,\n",
    "    verbose=True,\n",
    "    augment_data=False,\n",
    "    parent_path=\"..\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 23:14:19,030 - Local cache of dataset found, loading tokenized dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tokenized dataset\n"
     ]
    }
   ],
   "source": [
    "dl = dp.get_dataloader(batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Input shape: torch.Size([32, 128])\n",
      "Label shape: torch.Size([32, 128])\n",
      "Input sequence: : Army, Navy and Aerospace Force (Title VII, chapter VII, Art. 217)\n",
      "\n",
      "This is a subtle yet important distinction, both in terms of emphasizing the civil nature of the National Police, but also adapting the national police to function as a paramilitary force which can perform military duties as a result of the Colombian Conflict. This has led to some of the most important police units adopting military training and conducting special operations alongside the Colombian Army, Aerospace Force, and Navy. ThereforeThe history of Colombia includes the settlements and society by indigenous peoples, most notably, the Muisca Confederation, Quimbaya Civilization, and Tairona Chief\n",
      "Output sequence:  Army, Navy and Aerospace Force (Title VII, chapter VII, Art. 217)\n",
      "\n",
      "This is a subtle yet important distinction, both in terms of emphasizing the civil nature of the National Police, but also adapting the national police to function as a paramilitary force which can perform military duties as a result of the Colombian Conflict. This has led to some of the most important police units adopting military training and conducting special operations alongside the Colombian Army, Aerospace Force, and Navy. ThereforeThe history of Colombia includes the settlements and society by indigenous peoples, most notably, the Muisca Confederation, Quimbaya Civilization, and Tairona Chiefdoms\n",
      "Batch 2\n",
      "Input shape: torch.Size([32, 128])\n",
      "Label shape: torch.Size([32, 128])\n",
      "Input sequence: KPSZ is powered at 10,000 watts by day and 5,000 watts at night.  It uses a directional antenna with a six-tower array to protect other stations on 940 AM, a Canadian and Mexican clear channel frequency.  The transmitter is on 228th Avenue at Carpenter Street in Hartford.\n",
      "\n",
      "History\n",
      "\n",
      "KIOA: 1948–1957\n",
      "On January 15, 1948, the station signed on as KIOA. Its studios were located in the Onthank building at 10th and Mulberry in downtown Des Moines. The transmitter and towers are located two miles northeast of Hartford, Iowa. The owners\n",
      "Output sequence: PSZ is powered at 10,000 watts by day and 5,000 watts at night.  It uses a directional antenna with a six-tower array to protect other stations on 940 AM, a Canadian and Mexican clear channel frequency.  The transmitter is on 228th Avenue at Carpenter Street in Hartford.\n",
      "\n",
      "History\n",
      "\n",
      "KIOA: 1948–1957\n",
      "On January 15, 1948, the station signed on as KIOA. Its studios were located in the Onthank building at 10th and Mulberry in downtown Des Moines. The transmitter and towers are located two miles northeast of Hartford, Iowa. The owners of\n",
      "Batch 3\n",
      "Input shape: torch.Size([32, 128])\n",
      "Label shape: torch.Size([32, 128])\n",
      "Input sequence: ov finished first in his first international tournament, in Třinec, severalThe aspect ratio of a geometric shape is the ratio of its sizes in different dimensions. For example, the aspect ratio of a rectangle is the ratio of its longer side to its shorter side—the ratio of width to height, when the rectangle is oriented as a \"landscape\".\n",
      "\n",
      "The aspect ratio is most often expressed as two integer numbers separated by a colon (x:y), less commonly as a simple or decimal fraction. The values x and y do not represent actual widths and heights but, rather, the proportion between width and height.\n",
      "Output sequence:  finished first in his first international tournament, in Třinec, severalThe aspect ratio of a geometric shape is the ratio of its sizes in different dimensions. For example, the aspect ratio of a rectangle is the ratio of its longer side to its shorter side—the ratio of width to height, when the rectangle is oriented as a \"landscape\".\n",
      "\n",
      "The aspect ratio is most often expressed as two integer numbers separated by a colon (x:y), less commonly as a simple or decimal fraction. The values x and y do not represent actual widths and heights but, rather, the proportion between width and height. As\n",
      "Batch 4\n",
      "Input shape: torch.Size([32, 128])\n",
      "Label shape: torch.Size([32, 128])\n",
      "Input sequence:  provisions. Messengers came to O'Donnell from the Scots, whom he had before invited over, to inform him that they had arrived at Derry. And those who had come thither were Donnell Gorm Mac Donnell, and Mac Leod of Ara. O'Donnell then set out with a small number of his forces to hire them; and he left another large party of them with Maguire to assist him, and he ordered them to remain blockading the castle. When the Lord Justice, Sir William Fitzwilliam, had received intelligence that the warders of Enniskillen were in want of stores and\n",
      "Output sequence: . Messengers came to O'Donnell from the Scots, whom he had before invited over, to inform him that they had arrived at Derry. And those who had come thither were Donnell Gorm Mac Donnell, and Mac Leod of Ara. O'Donnell then set out with a small number of his forces to hire them; and he left another large party of them with Maguire to assist him, and he ordered them to remain blockading the castle. When the Lord Justice, Sir William Fitzwilliam, had received intelligence that the warders of Enniskillen were in want of stores andB\n",
      "Batch 5\n",
      "Input shape: torch.Size([32, 128])\n",
      "Label shape: torch.Size([32, 128])\n",
      "Input sequence:  an attack, he was invalided out of the war, and his football career looked in doubt. However, he managed to recover fully and returned to Luton Town after the end of the war, making himself known as a winger who could play on either flank. Hoar played over 150 league matches for Luton between 1919 and 1924, as they played in the Southern League and later the Third Division South.\n",
      "\n",
      "In late 1924, Hoar joined Arsenal for £3,000, making his debut against Cardiff City on 29 November 1924, and went on to make nineteen appearances that season; he also had trials with England but never made\n",
      "Output sequence:  attack, he was invalided out of the war, and his football career looked in doubt. However, he managed to recover fully and returned to Luton Town after the end of the war, making himself known as a winger who could play on either flank. Hoar played over 150 league matches for Luton between 1919 and 1924, as they played in the Southern League and later the Third Division South.\n",
      "\n",
      "In late 1924, Hoar joined Arsenal for £3,000, making his debut against Cardiff City on 29 November 1924, and went on to make nineteen appearances that season; he also had trials with England but never made it\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    x, y = next(iter(dl))\n",
    "    print(f\"Batch {i + 1}\")\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Label shape:\", y.shape)\n",
    "    print(f\"Input sequence: {tokenizer.decode(x[0].tolist())}\")\n",
    "    print(f\"Output sequence: {tokenizer.decode(y[0].tolist())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
