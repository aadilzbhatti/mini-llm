{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Assuming the text_prediction module is in the 'src' directory\n",
    "module_path = \"/Users/aadil/Documents/Workspace/ml-projects/wiki-llm/src/\"  # Replace with the correct path to your 'src' directory\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, DistributedSampler, Dataset as TDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "import os\n",
    "import pyarrow as pa\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "\n",
    "from text_prediction.utils import RankFilter\n",
    "\n",
    "class TokenizedDataset(TorchDataset):\n",
    "    def __init__(self, tokenized_dataset, block_size, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        tokenized_examples: List of tokenized sequences (each a list of token IDs).\n",
    "        block_size: Length of each input sequence.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self._data = [torch.tensor(example['input_ids'], dtype=torch.long) for example in tokenized_dataset if len(example['input_ids']) > block_size]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._data) - self.block_size  # Max index to sample from\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return a single sample (input and target sequences)\n",
    "        \"\"\"\n",
    "        example = self._data[idx]\n",
    "        ix = torch.randint(0, len(example) - self.block_size, (1,)).item()\n",
    "        input_seq = example[ix:ix + self.block_size]\n",
    "        label_seq = example[ix + 1:ix + self.block_size + 1]\n",
    "        return input_seq.to(self.device), label_seq.to(self.device)\n",
    "\n",
    "class DataPipeline:\n",
    "\n",
    "    def __init__(self, tokenizer, max_len, block_size, regenerate=False, num_samples=10000, verbose=False, augment_data=False, parent_path=\".\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.block_size = block_size\n",
    "        self.regenerate = regenerate\n",
    "        self.num_samples = num_samples\n",
    "        self.train_dataloader = None\n",
    "        self.val_dataloader = None\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.verbose = verbose\n",
    "        self.augment_data = augment_data\n",
    "        self.parent_path = parent_path\n",
    "\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')  # Update logging format\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.addFilter(RankFilter(0))\n",
    "        self.tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<ARTICLE_START>\", \"<ARTICLE_END>\"]})\n",
    "\n",
    "    def log(self, message, level=logging.INFO):\n",
    "        if self.verbose:\n",
    "            self.logger.log(level, message)\n",
    "\n",
    "    def _sanitize_text(self, text):\n",
    "        \"\"\"Sanitizes the text while preserving meaningful dashes, numerical values with units, and hyperlink text.\"\"\"\n",
    "        \n",
    "        # Preserve Markdown-style and HTML links by keeping only the text\n",
    "        text = re.sub(r'\\[([^\\]]+)\\]\\(http\\S+\\)', r'\\1', text)  # Handles Markdown links\n",
    "        text = re.sub(r'<a\\s+href=[\"\\']http\\S+[\"\\']>(.*?)</a>', r'\\1', text, flags=re.IGNORECASE)  # Handles HTML links\n",
    "\n",
    "        # Remove standalone URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "        # Ensure we preserve dashes in hyphenated place names (e.g., Indianapolis–Carmel–Anderson)\n",
    "        text = re.sub(r'(\\w)\\s*[-–]\\s*(\\w)', r'\\1–\\2', text)  # Normalize hyphens and remove unwanted spaces around them\n",
    "\n",
    "        # Keep valid numerical values with units (e.g., \"3.0 square miles (7.8 km2)\")\n",
    "        text = re.sub(r'(\\d+(\\.\\d+)?)\\s*([a-zA-Z²]+)', r'\\1 \\3', text)  # Ensures numbers and units stay together\n",
    "        text = re.sub(r'\\((\\d+(\\.\\d+)?\\s*[a-zA-Z²]+)\\)', r'(\\1)', text)  # Ensures parenthetical units remain intact\n",
    "\n",
    "        # Preserve valid year ranges (e.g., 1992-2002)\n",
    "        text = re.sub(r'(?<!\\d)(\\d{4})-(\\d{4})(?!\\d)', r'\\1-\\2', text)  # Ensure valid formatting\n",
    "\n",
    "        # Remove unwanted characters but keep punctuation, parentheses, and percentage signs\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s.,!?\\'\\\"()%-²]', '', text)\n",
    "\n",
    "        # Normalize spaces\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "    def _tokenize_function(self, examples):\n",
    "        \"\"\"Tokenizes the examples, adds special tokens, and optionally augments data.\"\"\"\n",
    "        \n",
    "        # Sanitize text data\n",
    "        sanitized_texts = [self._sanitize_text(text) for text in examples[\"text\"]]\n",
    "\n",
    "        # Add special tokens to the beginning and end of each text\n",
    "        texts_with_special_tokens = [\n",
    "            \"<ARTICLE_START>\" + text + \"<ARTICLE_END>\" for text in sanitized_texts\n",
    "        ]\n",
    "\n",
    "        tokenized = self.tokenizer(\n",
    "            texts_with_special_tokens,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "\n",
    "        if self.augment_data:\n",
    "            # Add data augmentation logic here\n",
    "            tokenized[\"input_ids\"] = self._augment(tokenized[\"input_ids\"])\n",
    "\n",
    "        return tokenized\n",
    "\n",
    "    def _augment(self, input_ids):\n",
    "        # Implement data augmentation logic\n",
    "        # For example, randomly mask some tokens\n",
    "        augmented = []\n",
    "        for ids in input_ids:\n",
    "            if random.random() < 0.1:  # 10% chance to mask a token\n",
    "                ids[random.randint(0, len(ids) - 1)] = self.tokenizer.mask_token_id\n",
    "            augmented.append(ids)\n",
    "        return augmented\n",
    "\n",
    "    def _get_tokenized_dataset(self):\n",
    "        tokenized_dataset_path = f\"{self.parent_path}/data/{self.tokenizer.name_or_path}/wiki/tokenized_augmented\" if self.augment_data else f\"{self.parent_path}/data/{self.tokenizer.name_or_path}/wiki/tokenized\"\n",
    "\n",
    "        if not os.path.exists(tokenized_dataset_path) or self.regenerate:\n",
    "            self.log(\"Local cache of dataset not found, downloading and tokenizing dataset...\")\n",
    "            # Load dataset (small subset of num_samples samples)\n",
    "            ds = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=\"train\")\n",
    "            ds = ds.select(range(self.num_samples))\n",
    "            # Select only the 'text' column\n",
    "            ds = ds.remove_columns([col for col in ds.column_names if col != \"text\"])\n",
    "            # Tokenize the dataset\n",
    "            ds = ds.map(self._tokenize_function, batched=True)\n",
    "            ds.save_to_disk(tokenized_dataset_path)\n",
    "        else:\n",
    "            self.log(\"Local cache of dataset found, loading tokenized dataset...\")\n",
    "            ds = load_from_disk(tokenized_dataset_path)\n",
    "        return ds\n",
    "\n",
    "    def _is_dataset_valid(self, dataset_path):\n",
    "        try:\n",
    "            ds = load_from_disk(dataset_path)\n",
    "            # Attempt to read a small portion of the dataset to ensure it's valid\n",
    "            _ = ds[:1]\n",
    "            return True\n",
    "        except (pa.lib.ArrowInvalid, FileNotFoundError):\n",
    "            return False\n",
    "\n",
    "    @staticmethod\n",
    "    def custom_collate(batch, tokenizer):\n",
    "        inputs, labels = zip(*batch)\n",
    "        pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "        inputs = pad_sequence(inputs, batch_first=True, padding_value=pad_token_id)\n",
    "        labels = pad_sequence(labels, batch_first=True, padding_value=pad_token_id)\n",
    "\n",
    "        # Create attention masks\n",
    "        B, T = inputs.shape\n",
    "        tril_mask = torch.tril(torch.ones(T, T)).bool().to(inputs.device) #move tril to the same device as inputs\n",
    "        attention_mask = (inputs != pad_token_id).unsqueeze(1).repeat(1, T, 1) & tril_mask #create a causal mask.\n",
    "        return inputs, labels, attention_mask.to(inputs.device)\n",
    "\n",
    "    def get_dataloader(self, batch_size, split=\"train\", shuffle=True):\n",
    "        # Ensure split is either \"train\" or \"val\"\n",
    "        if split not in [\"train\", \"val\"]:\n",
    "            raise ValueError(\"split must be either 'train' or 'val'\")\n",
    "        \n",
    "        # DataLoader with random sampling\n",
    "        tds = self._get_tokenized_dataset()\n",
    "        \n",
    "        # Split the dataset into train and validation sets\n",
    "        split_datasets = tds.train_test_split(test_size=0.2, seed=42)\n",
    "        selected_dataset = split_datasets[\"train\" if split == \"train\" else \"test\"]\n",
    "        \n",
    "        dataset = TokenizedDataset(selected_dataset, self.block_size, self.device)\n",
    "        \n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            collate_fn=lambda batch: DataPipeline.custom_collate(batch, self.tokenizer),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "dp = DataPipeline(tokenizer, max_len=512, block_size=128, regenerate=False, num_samples=10000, verbose=True, augment_data=False, parent_path=\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 08:27:49,687 - Local cache of dataset not found, downloading and tokenizing dataset...\n",
      "Map: 100%|██████████| 10000/10000 [00:16<00:00, 601.04 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 54725.28 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example 0 inputs\n",
      " According to the U.S. Census Bureau, Alexandria has a total area of , all land. Pipe Creek crosses the city south of its center, flowing southwest to join the White River at Perkinsville. Demographics Alexandria is part of the IndianapolisCarmelAnderson metropolitan statistical area. 2010 census As of the census of 2010, there were 5,145 people, 2,113 households, and 1,362 families living in the city. The population density was . There were 2,507 housing units at an average density of . The racial makeup of the city was 97.4% White, 0.3% African American, 0.\n",
      "example 0 labels\n",
      " to the U.S. Census Bureau, Alexandria has a total area of , all land. Pipe Creek crosses the city south of its center, flowing southwest to join the White River at Perkinsville. Demographics Alexandria is part of the IndianapolisCarmelAnderson metropolitan statistical area. 2010 census As of the census of 2010, there were 5,145 people, 2,113 households, and 1,362 families living in the city. The population density was . There were 2,507 housing units at an average density of . The racial makeup of the city was 97.4% White, 0.3% African American, 0.1\n",
      "example 1 inputs\n",
      " treatise on ritual magic, which appeared in two volumes between 1854 (Dogme) and 1856 (Rituel). Each volume is structured into 22 chapters, which parallel the tarot. Translations Lvi's Dogme et Rituel de la Haute Magie was translated into English by Arthur Edward Waite as Transcendental Magic, its Doctrine and Ritual (1896). Waite added a biographical preface and footnotes. A revised edition of this translation was published in 1923. A second translation, The Doctrine and Ritual of High Magic A New Translation, by John Michael Greer and Mark Mikituk\n",
      "example 1 labels\n",
      "ise on ritual magic, which appeared in two volumes between 1854 (Dogme) and 1856 (Rituel). Each volume is structured into 22 chapters, which parallel the tarot. Translations Lvi's Dogme et Rituel de la Haute Magie was translated into English by Arthur Edward Waite as Transcendental Magic, its Doctrine and Ritual (1896). Waite added a biographical preface and footnotes. A revised edition of this translation was published in 1923. A second translation, The Doctrine and Ritual of High Magic A New Translation, by John Michael Greer and Mark Mikituk,\n",
      "example 2 inputs\n",
      " Gnome\" in 1967. Bowie has said that the song was written for and about his half brother, Terry Burns, who had schizophrenia and was an inmate of Cane Hill Hospital (pictured on the original U.S. cover of The Man Who Sold the World) until his suicide in 1985. The lyrics include references to lobotomy, the tranquilizer Librium and EST, or Electroshock Therapy. Release and aftermath The second track on The Man Who Sold the World, \"All the Madmen\" was released by Mercury Records in edited form as a promo single (featuring the same song on both sides) in the U.S\n",
      "example 2 labels\n",
      "ome\" in 1967. Bowie has said that the song was written for and about his half brother, Terry Burns, who had schizophrenia and was an inmate of Cane Hill Hospital (pictured on the original U.S. cover of The Man Who Sold the World) until his suicide in 1985. The lyrics include references to lobotomy, the tranquilizer Librium and EST, or Electroshock Therapy. Release and aftermath The second track on The Man Who Sold the World, \"All the Madmen\" was released by Mercury Records in edited form as a promo single (featuring the same song on both sides) in the U.S.\n",
      "example 3 inputs\n",
      " Register of Historic Places in 1978. Campus The buildings on the knoll were built in 1839 and comprised a large three-story central building with columns and two wings. The wings, with 33 rooms each, were separated by 20 feet from the main building and connected to it via two-story covered passageways. The central building housed classrooms, offices, staff apartments, and dining facilities, a library and reading room on the second floor, and a large room on the third floor which might serve as a chapel, while the other two buildings served as separate male and female dormitories. The two-mile-long Lake Mosw\n",
      "example 3 labels\n",
      " of Historic Places in 1978. Campus The buildings on the knoll were built in 1839 and comprised a large three-story central building with columns and two wings. The wings, with 33 rooms each, were separated by 20 feet from the main building and connected to it via two-story covered passageways. The central building housed classrooms, offices, staff apartments, and dining facilities, a library and reading room on the second floor, and a large room on the third floor which might serve as a chapel, while the other two buildings served as separate male and female dormitories. The two-mile-long Lake Moswans\n",
      "example 4 inputs\n",
      " and television Capital (film), a 2012 French drama Capital (British TV series), a 2015 adaptation of Lanchester's novel Capital (Iranian TV series), 20112020 Capital TV, a former British rolling-music TV channel Capital TV (Belarus), a TV channel CTC (TV station), formerly called Capital Television, Ten Capital and Capital 7 Capital Television (New Zealand), a former television channel in Wellington, the capital of New Zealand, operated by TVNZ Music Capital (album), by Mick Softley, 1976 Capital (band), a British band \"Capital (It Fails Us Now)\", a song by Gang of Four from the\n",
      "example 4 labels\n",
      " television Capital (film), a 2012 French drama Capital (British TV series), a 2015 adaptation of Lanchester's novel Capital (Iranian TV series), 20112020 Capital TV, a former British rolling-music TV channel Capital TV (Belarus), a TV channel CTC (TV station), formerly called Capital Television, Ten Capital and Capital 7 Capital Television (New Zealand), a former television channel in Wellington, the capital of New Zealand, operated by TVNZ Music Capital (album), by Mick Softley, 1976 Capital (band), a British band \"Capital (It Fails Us Now)\", a song by Gang of Four from the 1982\n",
      "example 5 inputs\n",
      " but on February 16, 2006, she was disqualified from further competition for failing a drug test when she tested positive for the stimulant carphedon. The International Olympic Committee panel found her guilty, and she was expelled from the games and stripped of her medal. She was then banned for two years from competition, and the authorities in Turin started a criminal investigation into the matter. The head of the Russian Anti-Doping Committee claimed that Pyleva took an over-the-counter medication for an ankle injury prescribed by her personal doctor who is not a team doctor, which contained carphedon. Record Olympic Games World Championships See also Russia at\n",
      "example 5 labels\n",
      " on February 16, 2006, she was disqualified from further competition for failing a drug test when she tested positive for the stimulant carphedon. The International Olympic Committee panel found her guilty, and she was expelled from the games and stripped of her medal. She was then banned for two years from competition, and the authorities in Turin started a criminal investigation into the matter. The head of the Russian Anti-Doping Committee claimed that Pyleva took an over-the-counter medication for an ankle injury prescribed by her personal doctor who is not a team doctor, which contained carphedon. Record Olympic Games World Championships See also Russia at the\n"
     ]
    }
   ],
   "source": [
    "train_loader = dp.get_dataloader(32)\n",
    "\n",
    "# Get a batch\n",
    "i = 0\n",
    "for x, y, _ in train_loader:\n",
    "    print(f\"example {i} inputs\")\n",
    "    print(tokenizer.decode(x[0]))\n",
    "    print(f\"example {i} labels\")\n",
    "    print(tokenizer.decode(y[0]))\n",
    "    i += 1\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'text_prediction'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtext_prediction\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'text_prediction'"
     ]
    }
   ],
   "source": [
    "import text_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: text_prediction in /Users/aadil/Documents/Workspace/ml-projects/wiki-llm/.venv/lib/python3.13/site-packages (0.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --no-cache-dir text_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Assuming the text_prediction module is in the 'src' directory\n",
    "module_path = \"/Users/aadil/Documents/Workspace/ml-projects/wiki-llm/src/\"  # Replace with the correct path to your 'src' directory\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aadil/Documents/Workspace/ml-projects/wiki-llm/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from text_prediction.data_pipeline import DataPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "dp = DataPipeline(\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=512,\n",
    "    block_size=128,\n",
    "    num_samples=10000,\n",
    "    verbose=True,\n",
    "    augment_data=False,\n",
    "    parent_path=\"..\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 23:14:19,030 - Local cache of dataset found, loading tokenized dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tokenized dataset\n"
     ]
    }
   ],
   "source": [
    "dl = dp.get_dataloader(batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Input shape: torch.Size([32, 128])\n",
      "Label shape: torch.Size([32, 128])\n",
      "Input sequence: : Army, Navy and Aerospace Force (Title VII, chapter VII, Art. 217)\n",
      "\n",
      "This is a subtle yet important distinction, both in terms of emphasizing the civil nature of the National Police, but also adapting the national police to function as a paramilitary force which can perform military duties as a result of the Colombian Conflict. This has led to some of the most important police units adopting military training and conducting special operations alongside the Colombian Army, Aerospace Force, and Navy. ThereforeThe history of Colombia includes the settlements and society by indigenous peoples, most notably, the Muisca Confederation, Quimbaya Civilization, and Tairona Chief\n",
      "Output sequence:  Army, Navy and Aerospace Force (Title VII, chapter VII, Art. 217)\n",
      "\n",
      "This is a subtle yet important distinction, both in terms of emphasizing the civil nature of the National Police, but also adapting the national police to function as a paramilitary force which can perform military duties as a result of the Colombian Conflict. This has led to some of the most important police units adopting military training and conducting special operations alongside the Colombian Army, Aerospace Force, and Navy. ThereforeThe history of Colombia includes the settlements and society by indigenous peoples, most notably, the Muisca Confederation, Quimbaya Civilization, and Tairona Chiefdoms\n",
      "Batch 2\n",
      "Input shape: torch.Size([32, 128])\n",
      "Label shape: torch.Size([32, 128])\n",
      "Input sequence: KPSZ is powered at 10,000 watts by day and 5,000 watts at night.  It uses a directional antenna with a six-tower array to protect other stations on 940 AM, a Canadian and Mexican clear channel frequency.  The transmitter is on 228th Avenue at Carpenter Street in Hartford.\n",
      "\n",
      "History\n",
      "\n",
      "KIOA: 1948–1957\n",
      "On January 15, 1948, the station signed on as KIOA. Its studios were located in the Onthank building at 10th and Mulberry in downtown Des Moines. The transmitter and towers are located two miles northeast of Hartford, Iowa. The owners\n",
      "Output sequence: PSZ is powered at 10,000 watts by day and 5,000 watts at night.  It uses a directional antenna with a six-tower array to protect other stations on 940 AM, a Canadian and Mexican clear channel frequency.  The transmitter is on 228th Avenue at Carpenter Street in Hartford.\n",
      "\n",
      "History\n",
      "\n",
      "KIOA: 1948–1957\n",
      "On January 15, 1948, the station signed on as KIOA. Its studios were located in the Onthank building at 10th and Mulberry in downtown Des Moines. The transmitter and towers are located two miles northeast of Hartford, Iowa. The owners of\n",
      "Batch 3\n",
      "Input shape: torch.Size([32, 128])\n",
      "Label shape: torch.Size([32, 128])\n",
      "Input sequence: ov finished first in his first international tournament, in Třinec, severalThe aspect ratio of a geometric shape is the ratio of its sizes in different dimensions. For example, the aspect ratio of a rectangle is the ratio of its longer side to its shorter side—the ratio of width to height, when the rectangle is oriented as a \"landscape\".\n",
      "\n",
      "The aspect ratio is most often expressed as two integer numbers separated by a colon (x:y), less commonly as a simple or decimal fraction. The values x and y do not represent actual widths and heights but, rather, the proportion between width and height.\n",
      "Output sequence:  finished first in his first international tournament, in Třinec, severalThe aspect ratio of a geometric shape is the ratio of its sizes in different dimensions. For example, the aspect ratio of a rectangle is the ratio of its longer side to its shorter side—the ratio of width to height, when the rectangle is oriented as a \"landscape\".\n",
      "\n",
      "The aspect ratio is most often expressed as two integer numbers separated by a colon (x:y), less commonly as a simple or decimal fraction. The values x and y do not represent actual widths and heights but, rather, the proportion between width and height. As\n",
      "Batch 4\n",
      "Input shape: torch.Size([32, 128])\n",
      "Label shape: torch.Size([32, 128])\n",
      "Input sequence:  provisions. Messengers came to O'Donnell from the Scots, whom he had before invited over, to inform him that they had arrived at Derry. And those who had come thither were Donnell Gorm Mac Donnell, and Mac Leod of Ara. O'Donnell then set out with a small number of his forces to hire them; and he left another large party of them with Maguire to assist him, and he ordered them to remain blockading the castle. When the Lord Justice, Sir William Fitzwilliam, had received intelligence that the warders of Enniskillen were in want of stores and\n",
      "Output sequence: . Messengers came to O'Donnell from the Scots, whom he had before invited over, to inform him that they had arrived at Derry. And those who had come thither were Donnell Gorm Mac Donnell, and Mac Leod of Ara. O'Donnell then set out with a small number of his forces to hire them; and he left another large party of them with Maguire to assist him, and he ordered them to remain blockading the castle. When the Lord Justice, Sir William Fitzwilliam, had received intelligence that the warders of Enniskillen were in want of stores andB\n",
      "Batch 5\n",
      "Input shape: torch.Size([32, 128])\n",
      "Label shape: torch.Size([32, 128])\n",
      "Input sequence:  an attack, he was invalided out of the war, and his football career looked in doubt. However, he managed to recover fully and returned to Luton Town after the end of the war, making himself known as a winger who could play on either flank. Hoar played over 150 league matches for Luton between 1919 and 1924, as they played in the Southern League and later the Third Division South.\n",
      "\n",
      "In late 1924, Hoar joined Arsenal for £3,000, making his debut against Cardiff City on 29 November 1924, and went on to make nineteen appearances that season; he also had trials with England but never made\n",
      "Output sequence:  attack, he was invalided out of the war, and his football career looked in doubt. However, he managed to recover fully and returned to Luton Town after the end of the war, making himself known as a winger who could play on either flank. Hoar played over 150 league matches for Luton between 1919 and 1924, as they played in the Southern League and later the Third Division South.\n",
      "\n",
      "In late 1924, Hoar joined Arsenal for £3,000, making his debut against Cardiff City on 29 November 1924, and went on to make nineteen appearances that season; he also had trials with England but never made it\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    x, y = next(iter(dl))\n",
    "    print(f\"Batch {i + 1}\")\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Label shape:\", y.shape)\n",
    "    print(f\"Input sequence: {tokenizer.decode(x[0].tolist())}\")\n",
    "    print(f\"Output sequence: {tokenizer.decode(y[0].tolist())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
